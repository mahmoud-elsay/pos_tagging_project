{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Spam_SMS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                            Message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 1921),\n",
       " ('the', 1328),\n",
       " ('and', 968),\n",
       " ('for', 703),\n",
       " ('your', 677),\n",
       " ('have', 571),\n",
       " ('call', 559),\n",
       " ('are', 486),\n",
       " ('that', 470),\n",
       " ('but', 422),\n",
       " ('not', 410),\n",
       " ('can', 385),\n",
       " ('with', 379),\n",
       " ('will', 379),\n",
       " (\"i'm\", 377),\n",
       " ('get', 375),\n",
       " ('just', 365),\n",
       " ('this', 312),\n",
       " ('when', 283),\n",
       " ('from', 277),\n",
       " ('&lt;#&gt;', 276),\n",
       " ('all', 261),\n",
       " ('how', 254),\n",
       " ('what', 251),\n",
       " ('now', 247),\n",
       " ('like', 236),\n",
       " ('got', 235),\n",
       " ('know', 230),\n",
       " ('was', 230),\n",
       " ('free', 228),\n",
       " ('out', 220),\n",
       " ('come', 220),\n",
       " ('its', 208),\n",
       " ('then', 205),\n",
       " ('good', 201),\n",
       " ('send', 187),\n",
       " ('only', 184),\n",
       " ('want', 183),\n",
       " ('text', 175),\n",
       " ('time', 169),\n",
       " (\"i'll\", 168),\n",
       " ('love', 163),\n",
       " ('...', 163),\n",
       " ('going', 161),\n",
       " ('need', 157),\n",
       " ('about', 156),\n",
       " ('still', 151),\n",
       " ('one', 150),\n",
       " ('txt', 149),\n",
       " ('see', 145)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get most common words\n",
    "all_words = []\n",
    "\n",
    "for line in df[\"Message\"]:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if len(word) > 2:\n",
    "            all_words.append(word.lower())\n",
    "\n",
    "Counter(all_words).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # Make all words lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove  punction,number & spical char\n",
    "    text = re.sub(\"[^a-zA-z]\", \" \", text)\n",
    "\n",
    "    # Single char removel\n",
    "    text = re.sub(r\"\\s+[a-zA-z]\\s+\", \" \", text)\n",
    "\n",
    "    # Remove multi spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Make a sentence to toknize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove Stopwords\n",
    "    final_token = [i for i in tokens if i not in stop_words]\n",
    "\n",
    "    # Apply lemma\n",
    "    final_words = []\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for i in final_token:\n",
    "        if len(i) > 2:\n",
    "            word = lemma.lemmatize(i)\n",
    "            final_words.append(word)\n",
    "    return \" \".join(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing is done\n"
     ]
    }
   ],
   "source": [
    "df[\"Clean_Text\"] = df[\"Message\"].apply(lambda x: text_preprocessing(x))\n",
    "print(\"Text Preprocessing is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"Clean_Text\"]\n",
    "text = \" \".join(x.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('jurong', 'point'): 1, ('point', 'crazy'): 1, ('crazy', 'available'): 1, ('available', 'bugis'): 1, ('bugis', 'great'): 1, ('great', 'world'): 1, ('world', 'buffet'): 1, ('buffet', 'cine'): 1, ('cine', 'got'): 1, ('got', 'amore'): 1, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_counts(txt):\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    bigrams = nltk.bigrams(tokens)\n",
    "    return FreqDist(bigrams)\n",
    "\n",
    "\n",
    "bigrams = bigram_counts(text)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'entry': 2, 'say': 2, 'jurong': 1, 'point': 1, 'crazy': 1, 'available': 1, 'bugis': 1, 'great': 1, 'world': 1, 'buffet': 1, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigram_counts(txt):\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    return nltk.FreqDist(tokens)\n",
    "\n",
    "\n",
    "unigrams = unigram_counts(text)\n",
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('jurong', 'point') :  1.0\n",
      "('point', 'crazy') :  1.0\n",
      "('crazy', 'available') :  1.0\n",
      "('available', 'bugis') :  1.0\n",
      "('bugis', 'great') :  1.0\n",
      "('great', 'world') :  1.0\n",
      "('world', 'buffet') :  1.0\n",
      "('buffet', 'cine') :  1.0\n",
      "('cine', 'got') :  1.0\n",
      "('got', 'amore') :  1.0\n",
      "('amore', 'wat') :  1.0\n",
      "('wat', 'lar') :  1.0\n",
      "('lar', 'joking') :  1.0\n",
      "('joking', 'wif') :  1.0\n",
      "('wif', 'oni') :  1.0\n",
      "('oni', 'free') :  1.0\n",
      "('free', 'entry') :  1.0\n",
      "('entry', 'wkly') :  0.5\n",
      "('wkly', 'comp') :  1.0\n",
      "('comp', 'win') :  1.0\n",
      "('win', 'cup') :  1.0\n",
      "('cup', 'final') :  1.0\n",
      "('final', 'tkts') :  1.0\n",
      "('tkts', 'may') :  1.0\n",
      "('may', 'text') :  1.0\n",
      "('text', 'receive') :  1.0\n",
      "('receive', 'entry') :  1.0\n",
      "('entry', 'question') :  0.5\n",
      "('question', 'std') :  1.0\n",
      "('std', 'txt') :  1.0\n",
      "('txt', 'rate') :  1.0\n",
      "('rate', 'apply') :  1.0\n",
      "('apply', 'dun') :  1.0\n",
      "('dun', 'say') :  1.0\n",
      "('say', 'early') :  0.5\n",
      "('early', 'hor') :  1.0\n",
      "('hor', 'already') :  1.0\n",
      "('already', 'say') :  1.0\n",
      "('say', 'nah') :  0.5\n",
      "('nah', 'think') :  1.0\n",
      "('think', 'go') :  1.0\n",
      "('go', 'usf') :  1.0\n",
      "('usf', 'life') :  1.0\n",
      "('life', 'around') :  1.0\n",
      "('around', 'though') :  1.0\n",
      "probability =  0.0625\n"
     ]
    }
   ],
   "source": [
    "def bigram_probability(txt):\n",
    "    total_prop = 1.0\n",
    "    for bigram, count in bigrams.items():\n",
    "        bigram_prop = count / unigrams[bigram[0]]\n",
    "        print(bigram, \": \", bigram_prop)\n",
    "        total_prop = total_prop * bigram_prop\n",
    "    print(\"probability = \", total_prop)\n",
    "\n",
    "\n",
    "bigram_probability(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
